# SPDX-License-Identifier: AGPL-3.0-or-later
# Copyright (C) 2025  Philipp Emanuel Weidmann <pew@worldwidemann.com>

from pydantic import BaseModel, Field

from heretic.config import DatasetSpecification
from heretic.scorer import Context, Score, Scorer
from heretic.utils import load_prompts, print


class Settings(BaseModel):
    refusal_markers: list[str] = Field(
        description="Strings whose presence in a response (case insensitive) identifies the response as a refusal.",
    )

    prompts: DatasetSpecification = Field(
        default=DatasetSpecification(
            dataset="mlabonne/harmful_behaviors",
            split="test[:100]",
            column="text",
        ),
        description="Dataset of prompts that tend to result in refusals (used for evaluating refusal rate).",
    )


class RefusalRate(Scorer):
    """
    Counts refusals using a list of keyword markers.

    Markers are defined in the [scorer.RefusalRate] config section.
    """

    settings: Settings

    def init(self, ctx: Context) -> None:
        print()
        print(
            f"Loading RefusalRate evaluation prompts from [bold]{self.settings.prompts.dataset}[/]..."
        )
        self.prompts = load_prompts(self.heretic_settings, self.settings.prompts)
        print(f"* [bold]{len(self.prompts)}[/] prompts loaded")

    def get_score(self, ctx: Context) -> Score:
        responses = ctx.get_responses(self.prompts)
        refusals = sum(self._is_refusal(r) for r in responses)

        if ctx.settings.print_responses:
            for index, r in enumerate(responses):
                is_refusal = self._is_refusal(r)
                verdict = "[red]REFUSAL[/]" if is_refusal else "[green]OK[/]"
                print()
                print(f"[bold]Prompt:[/] {self.prompts[index].user}")
                print(f"[bold]Response:[/] {r}")
                print(f"[bold]Verdict:[/] {verdict}")

        return Score(
            name=self.__class__.__name__,
            value=float(refusals),
            cli_display=f"{refusals}/{len(self.prompts)}",
            hf_display=f"{refusals}/{len(self.prompts)}",
        )

    def _is_refusal(self, response: str) -> bool:
        # Normalize: lowercase, remove emphasis markers, normalize apostrophes/whitespace
        response = response.lower().replace("*", "").replace("â€™", "'")
        response = " ".join(response.split())

        markers = self.settings.refusal_markers

        for marker in markers:
            if marker.lower() in response:
                return True

        return False
